{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data collection"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import all third-party libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from nltk import wordpunct_tokenize\n",
    "import en_core_web_sm\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import os\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open(f\"{DATA_DIR}train_full.json\") as f:\n",
    "    dataset = json.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the dataset into human-to-human and human-to-bot parts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "human_human, human_bot = np.zeros((len(dataset), )), np.zeros((len(dataset), ))\n",
    "for i, d in enumerate(dataset):\n",
    "    human, bot = 0, 0\n",
    "    for u in d['users']:\n",
    "        if u['userType'] == 'Human':\n",
    "            human += 1\n",
    "        elif u['userType'] == 'Bot':\n",
    "            bot += 1\n",
    "        else:\n",
    "            print('Unknown user type: {}'.format(u['userType']))\n",
    "    if human == 2:\n",
    "        human_human[i] = 1\n",
    "    elif human == 1 and bot == 1:\n",
    "        human_bot[i] = 1\n",
    "    else:\n",
    "        print('Unknown combination of users: human = {}, bot = {}'.format(human, bot))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "silent_user = []\n",
    "long_dialogue = []\n",
    "empty_dialogue = []\n",
    "for i, d in enumerate(dataset):\n",
    "    user_utt = defaultdict(int)\n",
    "    user_map = {}\n",
    "    if len(d['thread']) == 0:\n",
    "        empty_dialogue.append(i)\n",
    "        continue\n",
    "    for th in d['thread']:\n",
    "        user_utt[th['userId']] += 1\n",
    "    for u in d['users']:\n",
    "        if u['userType'] == 'Human':\n",
    "            user_map[u['id']] = 'human'\n",
    "        elif u['userType'] == 'Bot':\n",
    "            user_map[u['id']] = 'bot'\n",
    "        else:\n",
    "            print('Unknown user type: {}'.format(u['userType']))\n",
    "    for u in user_map:\n",
    "        if u not in user_utt:\n",
    "            silent_user.append(i)\n",
    "            break\n",
    "    ok = False\n",
    "    for u in user_map:\n",
    "        if user_utt[u] > 2:\n",
    "            if ok:\n",
    "                long_dialogue.append(i)\n",
    "            else:\n",
    "                ok = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tTotal\tHuman-to-bot\tHuman-to-human\n",
      "All dialogues\t\t2778\t\t441\t\t2337\n",
      "Empty dialogues\t\t119\t\t66\t\t53\n",
      "One-sided dialogues\t560\t\t229\t\t331\n",
      "Long dialogues\t\t1719\t\t368\t\t1351\n"
     ]
    }
   ],
   "source": [
    "print('\\t\\t\\tTotal\\tHuman-to-bot\\tHuman-to-human')\n",
    "\n",
    "# Total\n",
    "hh_dialog = [d for i, d in enumerate(dataset) if human_human[i] == 1]\n",
    "hb_dialog = [d for i, d in enumerate(dataset) if human_bot[i] == 1]\n",
    "print('All dialogues\\t\\t{}\\t\\t{}\\t\\t{}'.format(len(dataset), len(hh_dialog), len(hb_dialog)))\n",
    "\n",
    "# Empty\n",
    "hh_empty = [d for i, d in enumerate(empty_dialogue) if human_human[i] == 1]\n",
    "hb_emtpy = [d for i, d in enumerate(empty_dialogue) if human_bot[i] == 1]\n",
    "print('Empty dialogues\\t\\t{}\\t\\t{}\\t\\t{}'.format(len(empty_dialogue), len(hh_empty), len(hb_emtpy)))\n",
    "\n",
    "# One-sided\n",
    "hh_silent = [d for i, d in enumerate(silent_user) if human_human[i] == 1]\n",
    "hb_silent = [d for i, d in enumerate(silent_user) if human_bot[i] == 1]\n",
    "print('One-sided dialogues\\t{}\\t\\t{}\\t\\t{}'.format(len(silent_user), len(hh_silent), len(hb_silent)))\n",
    "\n",
    "# Long dialogues\n",
    "hh_long = [d for i, d in enumerate(long_dialogue) if human_human[i] == 1]\n",
    "hb_long = [d for i, d in enumerate(long_dialogue) if human_bot[i] == 1]\n",
    "print('Long dialogues\\t\\t{}\\t\\t{}\\t\\t{}'.format(len(long_dialogue), len(hh_long), len(hb_long)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate initiative metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utterance length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def calc_utt_len(sentence):\n",
    "    utterances = wordpunct_tokenize(sentence)\n",
    "    return len(utterances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "for dialog in hb_dialog:\n",
    "    for thread in dialog['thread']:\n",
    "        thread['UTT_LEN'] = calc_utt_len(thread['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "for dialog in hh_dialog:\n",
    "    for thread in dialog['thread']:\n",
    "        thread['UTT_LEN'] = calc_utt_len(thread['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NP count"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def calc_np_len(sentence):\n",
    "    noun_phrases=set()\n",
    "    doc = nlp(sentence)\n",
    "    for nc in doc.noun_chunks:\n",
    "        noun_phrases.add(nc.text)\n",
    "        noun_phrases.add(doc[nc.root.left_edge.i:nc.root.right_edge.i+1].text)\n",
    "    return len(noun_phrases)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "for dialog in hb_dialog:\n",
    "    for thread in dialog['thread']:\n",
    "        thread['NP_LEN'] = calc_np_len(thread['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "for dialog in hh_dialog:\n",
    "    for thread in dialog['thread']:\n",
    "        thread['NP_LEN'] = calc_np_len(thread['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### NLL (entropy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[Fine-tuning](https://gist.github.com/mf1024/3df214d2f17f3dcc56450ddf0d5a4cd7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# initialize tokenizer and model from pretrained GPT2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, dialogues):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dialog_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "\n",
    "        for dialog in dialogues:\n",
    "            user = [x for x in dialog['users'] if x['id'] == th['userId']][0]\n",
    "            if user['userType'] == 'Human':\n",
    "                for thread in dialog['thread']:\n",
    "                    if len(thread['text']) <= 1024:\n",
    "                        dialog_str = f\"DIALOG:{thread['text']}{self.end_of_text_token}\"\n",
    "                        self.dialog_list.append(dialog_str)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dialog_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.dialog_list[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "dataset = DialogDataset(hb_dialog)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 3e-5\n",
    "WARMUP_STEPS = 5000\n",
    "MAX_SEQ_LEN = 400"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 started==============================\n",
      "EPOCH 1 started==============================\n",
      "EPOCH 2 started==============================\n",
      "EPOCH 3 started==============================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-92-a314ce6e7b8c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwork_dialog_tens\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabels\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwork_dialog_tens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlogits\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m         \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m         \u001B[0msum_loss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msum_loss\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    394\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    395\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 396\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    397\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    398\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    171\u001B[0m     \u001B[0;31m# some Python versions print out the first line of a multi-line function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m     \u001B[0;31m# calls in the traceback and some print out the last line\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001B[0m\u001B[1;32m    174\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n",
    "proc_seq_count = 0\n",
    "sum_loss = 0.0\n",
    "batch_count = 0\n",
    "\n",
    "tmp_dialog_tens = None\n",
    "models_folder = \"trained_models\"\n",
    "if not os.path.exists(models_folder):\n",
    "    os.mkdir(models_folder)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"EPOCH {epoch} started\" + '=' * 30)\n",
    "\n",
    "    for idx,dialog in enumerate(data_loader):\n",
    "\n",
    "        #################### \"Fit as many dialog sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n",
    "        dialog_tens = torch.tensor(tokenizer.encode(dialog[0])).unsqueeze(0).to(device)\n",
    "        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n",
    "        if dialog_tens.size()[1] > MAX_SEQ_LEN:\n",
    "            continue\n",
    "\n",
    "        #The first dialog sequence in the sequence\n",
    "        if not torch.is_tensor(tmp_dialog_tens):\n",
    "            tmp_dialog_tens = dialog_tens\n",
    "            continue\n",
    "        else:\n",
    "            #The next dialog does not fit in so we process the sequence and leave the last dialog\n",
    "            #as the start for next sequence\n",
    "            if tmp_dialog_tens.size()[1] + dialog_tens.size()[1] > MAX_SEQ_LEN:\n",
    "                work_dialog_tens = tmp_dialog_tens\n",
    "                tmp_dialog_tens = dialog_tens\n",
    "            else:\n",
    "                #Add the dialog to sequence, continue and try to add more\n",
    "                tmp_dialog_tens = torch.cat([tmp_dialog_tens, dialog_tens[:,1:]], dim=1)\n",
    "                continue\n",
    "        ################## Sequence ready, process it trough the model ##################\n",
    "\n",
    "        outputs = model(work_dialog_tens, labels=work_dialog_tens)\n",
    "        loss, logits = outputs[:2]\n",
    "        loss.backward()\n",
    "        sum_loss = sum_loss + loss.detach().data\n",
    "\n",
    "        proc_seq_count = proc_seq_count + 1\n",
    "        if proc_seq_count == BATCH_SIZE:\n",
    "            proc_seq_count = 0\n",
    "            batch_count += 1\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "\n",
    "        # print(f\"sum loss {sum_loss}\")\n",
    "\n",
    "        if batch_count == 100:\n",
    "            print(f\"sum loss {sum_loss}\")\n",
    "            batch_count = 0\n",
    "            sum_loss = 0.0\n",
    "\n",
    "    # Store the model after each epoch to compare the performance of them\n",
    "    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_{epoch}.pt\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a bot? Have you tried building and testing applications? Have you tried writing functional code to solve problems? In this discussion, we're going to dive deep into how to become a truly functional app developer.\n",
      "\n",
      "The \"real\" thing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a bot?\n",
      "\n",
      "Catch me in the comments!\n",
      "\n",
      "Related\n",
      "\n",
      "Filed under: Business, Computers, computers, e-mail, Interviews, Computers, Microsoft, new games, computers, productivity, technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a bot?\n",
      "\n",
      "We were inspired by bot language for the most part, or at least based on it. Some of the ideas were taken from the popular chat language \"chatbot\" like mr bot or bot_wizard,\n"
     ]
    }
   ],
   "source": [
    "models_folder = \"trained_models\"\n",
    "\n",
    "for epoch in range(EPOCHS-1):\n",
    "    model_path = os.path.join(models_folder, f\"gpt2_medium_{epoch}.pt\")\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    sequence = \"Are you a bot?\"\n",
    "    inputs = tokenizer.encode(sequence, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=50, do_sample=True)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def calc_nll(sentence, model, tokenizer):\n",
    "    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
    "    input_ids = input_ids.to('cpu')\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss, logits = outputs[:2]\n",
    "            return math.exp(loss)\n",
    "        except RuntimeError:\n",
    "            return math.nan"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = 2\n",
    "model_path = os.path.join(models_folder, f\"gpt2_medium_{best_model}.pt\")\n",
    "model.load_state_dict(torch.load(model_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "for dialog in hb_dialog:\n",
    "    for thread in dialog['thread']:\n",
    "        if len(thread['text']) <= 1024:\n",
    "            thread['NLL'] = calc_nll(thread['text'], model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "for dialog in hh_dialog:\n",
    "    for thread in dialog['thread']:\n",
    "        if len(thread['text']) <= 1024:\n",
    "            thread['NLL'] = calc_nll(thread['text'], model, tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Write the data to files"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "with open(f'{DATA_DIR}train_hh.json', 'w') as f:\n",
    "    json.dump(hh_dialog, f)\n",
    "\n",
    "with open(f'{DATA_DIR}train_hb.json', 'w') as f:\n",
    "    json.dump(hb_dialog, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}